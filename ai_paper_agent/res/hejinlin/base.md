# FedAA: A Reinforcement Learning Perspective on Adaptive Aggregation for Fair and Robust Federated Learning

## 1. Problem Definition
The paper addresses the dual challenge of **Robustness** and **Fairness** in Federated Learning (FL) under Non-IID data distributions and Byzantine attacks.

- **Robustness Gap**: Traditional robust aggregation rules (e.g., Krum, Median) often degrade performance in benign settings or fail to handle sophisticated attacks while maintaining high accuracy.
- **Fairness Gap**: Personalized FL methods (e.g., Ditto, lp-proj) focus on client-side personalization ($w_k$) to handle heterogeneity but often use standard aggregation (e.g., FedAvg) at the server ($w_g$), leaving the global model vulnerable to poisoning which indirectly hurts personalization.
- **Core Conflict**: There is often a trade-off between robustness (filtering out outliers) and fairness (including diverse/outlier clients). The paper aims to optimize this trade-off dynamically.

**Formal Definitions:**
- **Robustness**: A model $w_1$ is more robust than $w_2$ if it achieves higher mean test accuracy across benign clients under Byzantine attacks.
- **Performance Fairness**: A model $w_1$ is fairer than $w_2$ if the standard deviation of test performance across $N$ clients is lower: $\text{std}\{F_k(w_1)\} < \text{std}\{F_k(w_2)\}$.

## 2. Core Methodology
The authors propose **FedAA**, which models the server-side aggregation as a Markov Decision Process (MDP) and solves it using Deep Deterministic Policy Gradient (DDPG).

### 2.1. Bi-level Optimization Objective
The goal is to find a global model $w_g$ that maximizes accuracy on a server-side fair validation set $D_g$, while clients optimize local losses:
$$ \max_{w_g} \text{Acc}(w_g, D_g) $$
$$ \text{s.t. } w_g = \sum_{k=1}^N a_k w_k, \quad \sum a_k = 1, \quad a_k \ge 0 $$
where $w_k$ are locally optimized models.

### 2.2. Client Selection (State Generation)
To filter potential attackers and generate a compact state for RL, FedAA uses a distance-based heuristic:
1.  **Distance Matrix**: Compute Euclidean distance between flattened parameters of all clients: $C_{i,j} = \|w_i - w_j\|^2$.
2.  **Outlier Score**: Calculate sum of distances for each client: $d_i = \sum_j C_{i,j}$.
3.  **Selection**: Select top $M\%$ clients with the smallest $d_i$. Let this set be $\mathcal{S}_t$.
4.  **State $s_t$**: The normalized distance vector of the selected clients: $s_t = [d_{(1)}, \dots, d_{(M)}]$.

### 2.3. DRL-based Adaptive Aggregation
The server acts as an agent using DDPG:
- **State**: $s_t$ (as defined above).
- **Action**: Continuous aggregation weights $a_t \in \mathbb{R}^{|\mathcal{S}_t|}$ generated by the Actor network $\pi(s|\theta_\pi)$.
  $$ a_t = \text{Softmax}(\pi(s_t)) \quad (\text{Implicitly ensuring } \sum a_i = 1) $$
- **Reward**: The accuracy of the aggregated model on the server's held-out dataset $D_g$:
  $$ r_t = \text{Acc}\left(\sum_{i \in \mathcal{S}_t} a_{t,i} w_{t,i}, D_g\right) $$
- **Critic**: Estimates $Q(s, a|\theta_Q)$ to guide the Actor.

### 2.4. Workflow
1.  Clients train locally for $R$ rounds.
2.  Server computes distances, selects top $M\%$ clients.
3.  Server (Actor) predicts weights $a_t$.
4.  Server aggregates $w_g = \sum a_i w_i$.
5.  Server evaluates $w_g$ on $D_g$ to get reward $r_t$.
6.  Update Actor/Critic using DDPG updates.

## 3. Theoretical Proofs
The paper is primarily empirical. It relies on the **Universal Approximation Theorem** implicitly for the DRL agent's ability to learn the optimal aggregation policy.
- **Assumption**: The existence of a small, fair, clean dataset $D_g$ at the server is crucial. The paper assumes this proxy data is sufficient to guide the global model towards a robust and fair optimum.
- **Robustness Intuition**: By selecting clients with minimum aggregate distance to others, the algorithm assumes benign clients form a tight cluster while malicious clients (Byzantine) are outliers.

## 4. Experimental Setup
### 4.1. Datasets
- **Real-world**: MNIST, CIFAR10, FASHION-MNIST, EMNIST.
- **Synthetic**: SYNTHETIC(0,0) and SYNTHETIC(1,1) (following Shamir et al.).
- **Heterogeneity**: Dirichlet distribution $\alpha=0.1$ (High Non-IID).
- **Server Data**: 100 images per class for real datasets.

### 4.2. Attack Models
- **Same-value attacks**: $w_k = m \cdot \mathbf{1}$.
- **Sign-flipping attacks**: $w_k = -|m| w'_k$.
- **Gaussian attacks**: $w_k \sim \mathcal{N}(0, \tau^2 I)$.
- **IPM attacks**: Inner Product Manipulation (specifically against Krum).

### 4.3. Baselines
- **FedAvg**: Standard averaging.
- **Ditto**: Multi-task learning objective for personalization.
- **lp-proj**: Projecting updates to low-dimensional subspace.

### 4.4. Metrics
- **Test Accuracy**: Mean accuracy across benign clients.
- **Fairness**: Standard deviation of test accuracy across clients.

## 5. Implementation Details
- **Hyperparameters**:
  - Client Selection Ratio $M\%$: Typically 30% - 80% (Trade-off parameter).
  - RL Learning Rate: $10^{-2}$ for Actor/Critic.
  - Discount Factor $\gamma$: 0.99.
  - Soft Update $\tau$: 0.001.
- **Network Architecture**:
  - Actor/Critic: MLP with hidden sizes [256, 128, 64] (or compressed versions).
  - Client Models: CNNs for image datasets, Logistic Regression for synthetic.
- **Hardware**: NVIDIA L40S GPU.
